{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SHAP with XGBoost\n",
    "\n",
    "In this notebook, we will use the Titanic dataset to predict the survival of passengers using XGBoost. We will use SHAP to explain the predictions of the models.\n",
    "\n",
    "### Install and import the necessary libraries\n",
    "\n",
    "If you're running this on Google Colab, you'll need to install Dalex and SHAP but the other libraries should already be installed. If you're running this on your local machine, you'll need to install all the libraries needed."
   ],
   "id": "7418d848fe06366f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install the necessary libraries\n",
    "\n",
    "# !pip install -q dalex xgboost shap"
   ],
   "id": "ab814e0a452ef99c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import the necessary libraries\n",
    "import dalex as dx\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "ba639eb34f302358",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Load the Titanic dataset\n",
    "\n",
    "The Titanic dataset is a popular toy dataset for machine learning. It contains records about passengers on the Titanic, including whether they survived (i.e. `survived` = 1) or not (i.e. `survived` = 0). While the larger dataset contains more information, we are using a smaller version that only contains these specific columns:\n",
    "\n",
    "- `gender`: passengers' recorded gender.\n",
    "- `age`: passengers' age in years.\n",
    "- `class`: passengers' ticket class on the Titanic, which can be 1st, 2nd, or 3rd class, or a number of crew-specific classes.\n",
    "- `embarked`: the port where the passenger embarked from, which can be Cherbourg, Queenstown, or Southampton.\n",
    "- `fare`: how much the passenger paid for their ticket in British pounds.\n",
    "- `sibsp`: the number of siblings or spouses the passenger had on board.\n",
    "- `parch`: the number of parents or children the passenger had on board.\n",
    "- `survived`: whether the passenger survived or not."
   ],
   "id": "1a3927bcb50871ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = dx.datasets.load_titanic()\n",
    "df.head()"
   ],
   "id": "9e625655b3f89275",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can get some programmatic information about the dataset using the `info` method. In our uses, we will need to be careful about how we handle the categorical columns, which are the columns with the 'object' dtype. In our case, this is the `gender`, `class`, and `embarked` columns. For `gender`, we will shortly be replacing this with a 0-1 encoding. For `class` and `embarked`, we will convert the columns to a 'category' dtype so that they can be used in the model. Later we will need to use one-hot encoding for these features, where each possible value gets its own column.",
   "id": "2b0a12eaae8dd813"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.info()",
   "id": "9d4657ef6c3e2a30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The columns labelled 'object' as their dtype are categorical columns. We can convert them into a 'category' dtype so that they can be used in the model.",
   "id": "aa574d2f74643c3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['class'] = df['class'].astype('category')\n",
    "df['embarked'] = df['embarked'].astype('category')"
   ],
   "id": "68046e2758cde7ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.info()",
   "id": "1323483642a029f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Split the data into features and target\n",
    "\n",
    "We will also use the 'get_dummies' method to convert gender to a 0-1 encoding."
   ],
   "id": "a5e615c93ec35a0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "X = pd.get_dummies(X, columns=['gender'], drop_first=True)"
   ],
   "id": "a826de3c3157c43a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X.head()",
   "id": "7e9afdb02592d893",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Split the data into training and testing sets\n",
    "\n",
    "As always in machine learning, we need to hold back some of our data for testing. We will use 80% of the data for training and 20% for testing. We fix the random state so that the results are reproducible - this means that our randomness is not truly random, but it is consistent across runs."
   ],
   "id": "eac6d4b9ca6b87a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "3d62ebdccd75c503",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create our XGBoost model\n",
    "\n",
    "While we won't be covering XGBoost in this tutorial, it's a popular machine learning library that implements gradient boosting. In a nutshell, this is a type of ensemble learning where a series of decision trees are trained sequentially, with each tree trying to correct the errors of the previous tree. This is a powerful technique that can be used for both regression and classification tasks."
   ],
   "id": "58e6b0627a15f779"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = xgb.XGBClassifier(\n",
    "    n_estimators = 200,        # Number of trees to fit\n",
    "    max_depth = 4,             # Maximum tree depth for individual trees\n",
    "    use_label_encoder = False, # Leave this as False to avoid warnings\n",
    "    enable_categorical = True,  # Leave this as True to use categorical columns\n",
    "    tree_method = 'hist'       # Use a histogram-based method for faster training\n",
    ")"
   ],
   "id": "5f0a8b483c8ce9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.fit(X_train, y_train)",
   "id": "f804e3b798b6833",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can quickly check the accuracy of our model using the `score` method. This method calculates the accuracy of the model on the test set. You should expect to see a bit under 80% accuracy.",
   "id": "16dfec7c4da21a36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.score(X_test, y_test)",
   "id": "c797a605266a91e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Explain with Dalex\n",
    "\n",
    "[Dalex](https://dalex.drwhy.ai/) is a library that helps you understand how machine learning models work. It provides tools for understanding the model's performance, the importance of features, and how the model makes predictions. We can use Dalex to explain the predictions of our XGBoost model.\n",
    "\n",
    "We are going to create an instance of the `Explainer` class, which gives us access to a range of methods for understanding the model. To create this, we need to specify how the explainer can get predictions from the model. It expects to receive a single value for each entry that corresponds to the likelihood of the positive class, so that's exactly what our code does. We also specify the label of the model as 'XGBoost'."
   ],
   "id": "9eda2c8d4eec2e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def pf_xgboost_classifier_categorical(model, df):\n",
    "    # Make absolutely sure that the categorical columns are of type 'category'\n",
    "    df.loc[:, df.dtypes == 'object'] = \\\n",
    "        df.select_dtypes(['object']) \\\n",
    "            .apply(lambda x: x.astype('category'))\n",
    "    # Predict the probability of the positive class\n",
    "    return model.predict_proba(df)[:, 1]\n",
    "\n",
    "explainer = dx.Explainer(model, X, y, predict_function=pf_xgboost_classifier_categorical, label='XGBoost')"
   ],
   "id": "669ed5ffb40e1958",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can do a range of useful things with our explainer now, including getting an overview of our model's performance:",
   "id": "ec4c334f1b0a0de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "explainer.model_performance()",
   "id": "8828dde12b9f70fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "...making predictions:",
   "id": "2db64e695263cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "explainer.predict(X_test[0:10])",
   "id": "69d94a37cd621fb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "...and most important for this lab, explaining the predictions of the model. Here we will get the explanations using SHAP for the first 5 passengers in the data.",
   "id": "409c2480fa9b6679"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "shap_attributions = [explainer.predict_parts(X.iloc[[i]], type=\"shap\", label=f'passenger {i}') for i in range(5)]",
   "id": "8dd4454f1166f63d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "shap_attributions[0].plot(shap_attributions[1::])",
   "id": "26d0d6a57153f36c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The above plots show us each feature's contribution to the final prediction for that instance. Below we will see a different visualization of the same data, where this time we track how the prediction changes as we add more features.",
   "id": "629373fea1d30faf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bd_attributions = [explainer.predict_parts(X.iloc[[i]], type=\"break_down\", label=f'passenger {i}') for i in range(5)]",
   "id": "a72077ba4a28aa23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bd_attributions[0].plot(bd_attributions[1::])",
   "id": "67cb96a7756a485c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Explain with SHAP\n",
    "\n",
    "In addition to the Dalex library we used above, we can also use the original SHAP library to make the same type of interpretations. Here we will need to use one-hot encoding instead of our categorical dtype. The code below handles this for us.\n",
    "\n",
    "Note that `drop_first=True` means that we don't have a column for one of the possible values - instead, this value is inferred by having `False` in all the other columns for that feature. This improves our data efficiency slightly, but is a problem if we have missing data (since that would also be represented by a series of `False` values). Luckily, we have no missing data in this version of the Titanic dataset, although the larger version does have missing data."
   ],
   "id": "bbdfadd56d361afc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Need to convert the categorical columns to one-hot encoding\n",
    "X_ohe = pd.get_dummies(X, columns=['class', 'embarked'], drop_first=True)"
   ],
   "id": "f3879b162dc9c1f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_ohe.head()",
   "id": "cc6e77b5559d9917",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's build another classifier using the one-hot encoded data, for us to examine the SHAP values.",
   "id": "30c8f824997478fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_ohe, X_test_ohe, y_train, y_test = train_test_split(X_ohe, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_ohe = xgb.XGBClassifier(\n",
    "    n_estimators = 200,        # Number of trees to fit\n",
    "    max_depth = 4,             # Maximum tree depth for individual trees\n",
    "    use_label_encoder = False, # Leave this as False to avoid warnings\n",
    "    tree_method = 'hist'       # Use a histogram-based method for faster training\n",
    ")"
   ],
   "id": "9aec5a7afa1d3a4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_ohe.fit(X_train_ohe, y_train)",
   "id": "1f5caac7b48448a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_ohe.score(X_test_ohe, y_test)",
   "id": "22e56f7ef4a7a15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Interestingly, this model performs ever so slightly better than the previous model. Could this be due to a difference in the random split? Or is it because the one-hot encoding is more effective for this dataset and model combination? We can't say for sure, but it's worth considering.",
   "id": "a20d44f2ac0aa984"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The SHAP explainer really doesn't like converting datatypes for us, so we are converting everything to a float here\n",
    "explainer_ohe = shap.explainers.Tree(model_ohe, data=X_train_ohe.astype('float64'), model_output='probability')"
   ],
   "id": "d86d8d176bd969d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "shap_values = explainer_ohe.shap_values(X_test_ohe)",
   "id": "764478d81670c55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "shap.summary_plot(shap_values, X_test_ohe)",
   "id": "cb927027cd6be051",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(5):\n",
    "    shap.force_plot(explainer_ohe.expected_value, shap_values[i], X_test_ohe.iloc[i], matplotlib=True)"
   ],
   "id": "ae897c4449cd916b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
