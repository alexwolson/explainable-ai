{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fairness with XGBoost\n",
    "\n",
    "In this final mini-lab, we'll look at some methods for evaluating the fairness of a model when considering some protected characteristic.\n",
    "\n",
    "In our case, we will use the `gender` class as our protected variable, and set the `male` group as the privileged one. This is flawed in this specific case, because the actual historical behaviour was biased by gender, and so a model should actually factor this in. However, for the sake of this lab, we will assume that the model should not be biased by gender."
   ],
   "id": "6a74e87073f9dfca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install the necessary libraries\n",
    "\n",
    "# !pip install -q dalex xgboost lime"
   ],
   "id": "b841fdfdcf7a4683",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import dalex as dx\n",
    "import xgboost\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load data",
   "id": "239d21b0cf928d3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = dx.datasets.load_titanic()\n",
    "\n",
    "X = df.drop(columns='survived')\n",
    "X = pd.get_dummies(X, columns=[\"gender\", \"class\", \"embarked\"], drop_first=True)\n",
    "y = df.survived\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split( X, y, test_size=0.33, random_state=42)"
   ],
   "id": "44912799cc8095a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = xgboost.XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=2,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "\n",
    "    enable_categorical=True,\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ],
   "id": "f470d2efd7e78519",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def pf_xgboost_classifier_categorical(model, df):\n",
    "    df.loc[:, df.dtypes == 'object'] = \\\n",
    "        df.select_dtypes(['object']) \\\n",
    "            .apply(lambda x: x.astype('category'))\n",
    "    return model.predict_proba(df)[:, 1]\n",
    "\n",
    "explainer = dx.Explainer(model, X_test, y_test, predict_function=pf_xgboost_classifier_categorical)"
   ],
   "id": "2fcf07188899c8a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "explainer.model_performance()",
   "id": "da921ab325b70cb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fairness\n",
    "\n",
    "Now we will look at creating a protected group in our dataset, to evaluate the fairness of our model. We need to define the protected variable and the privileged group, where the protected variable is the value of gender for each entry, and the privileged value is male."
   ],
   "id": "4ac3cffd3d49960e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "protected_variable = X_test.gender_male.apply(lambda x: \"male\" if x else \"female\")\n",
    "privileged_group = \"male\"\n",
    "\n",
    "fobject = explainer.model_fairness(\n",
    "    protected=protected_variable,\n",
    "    privileged=privileged_group\n",
    ")"
   ],
   "id": "10dea85645765001",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Bias detection\n",
    "\n",
    "Fairness objects have a convenient form of describing model bias using the fairness_check() method.\n",
    "\n",
    "Several metrics are computed and checked automatically.\n",
    "\n",
    "- TPR - True positive rate / Equal opportunity\n",
    "- PPV - Positive predictive value / Predictive parity\n",
    "- FPR - False positive rate / Predictive equality\n",
    "- ACC - Accuracy / Accuracy equality \n",
    "- STP - Statistical parity / Demographic parity \n",
    "\n",
    "For a broad description of these methods, consider refering to the following article and its references:\n",
    "\n",
    "    J. Wi≈õniewski & P. Biecek. fairmodels: a Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models. The R Journal, 2022.\n",
    "\n",
    "More resources are available at https://fairmodels.drwhy.ai and specifically for Python at https://dalex.drwhy.ai/python#fairness.\n"
   ],
   "id": "ee5c14b349fe70a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fobject.fairness_check()",
   "id": "62e30ef26a6e0254",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fobject.plot()",
   "id": "d183ffc46c515e85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We clearly observe high bias towards the privileged group in the model. This is not surprising, as the model was trained on a dataset that was biased towards the privileged group. Let's see what happens if we train a model without the protected attribute. Note that this is not a suitable practice in the real world, as it doesn't ensure the model is unbiased. For example, in the full Titanic dataset there is a feature for the names of passengers. A model could use the title associated with each passenger (prof, ms, lord, etc.) to infer gender in many cases, producing the same bias.",
   "id": "a3ebc61bd4fefd42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_without_prot, X_test_without_prot = X_train.drop(\"gender_male\", axis=1), X_test.drop(\"gender_male\", axis=1)\n",
    "\n",
    "model_without_prot = xgboost.XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=2,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "    enable_categorical=True,\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "\n",
    "model_without_prot.fit(X_train_without_prot, y_train)\n",
    "\n",
    "explainer_without_prot = dx.Explainer(\n",
    "    model_without_prot,\n",
    "    X_test_without_prot,\n",
    "    y_test,\n",
    "    predict_function=pf_xgboost_classifier_categorical,\n",
    "    label=\"XGBClassifier without the protected attribute\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "fobject_without_prot = explainer_without_prot.model_fairness(protected_variable, privileged_group)"
   ],
   "id": "3481e9f9ab63b752",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fobject.plot(fobject_without_prot, show=False). \\\n",
    "    update_layout(autosize=False, width=800, height=450, legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99))"
   ],
   "id": "8be789c6c9079075",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We managed to improve on 3 fairness metrics, at a cost of worse Predictive parity ratio.\n",
    "\n",
    "This comes at a cost of model performance:"
   ],
   "id": "ac57ecf117ba15da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pd.concat([explainer.model_performance().result, explainer_without_prot.model_performance().result], axis=0)",
   "id": "1c0efd7500670d17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This phenomenon is known as the bias-performance tradeoff. Evaluating this tradeoff is very complex, with no one-size-fits-all solution. Deciding whether a given bias is acceptable is a domain-specific decision and should ultimately be made by a person or group with a deep understanding of the problem.\n",
    "\n",
    "### Bias mitigation\n",
    "\n",
    "Can we decrease model bias without decreasing model performance?\n",
    "\n",
    "This is the goal of bias mitigation methods:\n",
    "\n",
    "- resample: resample the dataset to balance the occurrence of the protected variable\n",
    "- reweight: reweight the samples so that lower-represented groups have more weight in evaluation\n",
    "- roc_pivot: change the decision threshold for different groups to balance the model's performance\n",
    "\n",
    "Dalex makes it easy to apply these methods to a model and evaluate the results.\n"
   ],
   "id": "c550816c34ec0005"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dalex.fairness import resample, reweight, roc_pivot\n",
    "from copy import copy\n",
    "\n",
    "def create_explainer(model, X_train, y_train, X_test, y_test, label):\n",
    "    model.fit(X_train, y_train)\n",
    "    return dx.Explainer(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        label=label,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "protected_variable_train = X_train.gender_male.apply(lambda x: \"male\" if x else \"female\")\n",
    "\n",
    "# Resample\n",
    "indices_resample = resample(\n",
    "    protected_variable_train,\n",
    "    y_train,\n",
    "    type='preferential',  # uniform\n",
    "    probs=model_without_prot.predict_proba(X_train_without_prot)[:, 1],  # requires probabilities\n",
    "    verbose=False\n",
    ")\n",
    "model_resample = copy(model_without_prot)\n",
    "explainer_resample = create_explainer(\n",
    "    model_resample,\n",
    "    X_train_without_prot.iloc[indices_resample, :],\n",
    "    y_train.iloc[indices_resample],\n",
    "    X_test_without_prot,\n",
    "    y_test,\n",
    "    label='XGBClassifier with Resample mitigation'\n",
    ")\n",
    "fobject_resample = explainer_resample.model_fairness(protected_variable, privileged_group)\n",
    "\n",
    "# Reweight\n",
    "sample_weight = reweight(\n",
    "    protected_variable_train,\n",
    "    y_train,\n",
    "    verbose=False\n",
    ")\n",
    "model_reweight = copy(model_without_prot)\n",
    "explainer_reweight = create_explainer(\n",
    "    model_reweight,\n",
    "    X_train_without_prot,\n",
    "    y_train,\n",
    "    X_test_without_prot,\n",
    "    y_test,\n",
    "    label='XGBClassifier with Reweight mitigation'\n",
    ")\n",
    "fobject_reweight = explainer_reweight.model_fairness(protected_variable, privileged_group)\n",
    "\n",
    "# ROC Pivot\n",
    "explainer_roc_pivot = roc_pivot(\n",
    "    copy(explainer_without_prot),\n",
    "    protected_variable,\n",
    "    privileged_group,\n",
    "    verbose=False\n",
    ")\n",
    "explainer_roc_pivot.label = 'XGBClassifier with ROC pivot mitigation'\n",
    "fobject_roc_pivot = explainer_roc_pivot.model_fairness(protected_variable, privileged_group)"
   ],
   "id": "6eeb58790766bfb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fobject_without_prot.plot([fobject_resample, fobject_reweight, fobject_roc_pivot], show=False). \\\n",
    "    update_layout(autosize=False, width=800, height=450, legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99))"
   ],
   "id": "2c666a3aff3bb117",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for fobj in [fobject_without_prot, fobject_resample, fobject_reweight, fobject_roc_pivot]:\n",
    "    print(\"\\n========== \" + fobj.label + \" ==========\")\n",
    "    fobj.fairness_check(epsilon=0.66)"
   ],
   "id": "cea406699b360a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that even though we have removed the protected variable and attempted to improve fairness, we are still outside of the acceptable range for many metrics. Again, this dataset is a poor example since we are punishing the model for not being biased towards a group that really was privileged in the real world. This is a good example of why it is important to understand the domain and the data when evaluating fairness.\n",
    "\n",
    "Finally, let's compare the model performance across our different models:"
   ],
   "id": "2b310015004e87ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.concat([\n",
    "    explainer_without_prot.model_performance().result,\n",
    "    explainer_resample.model_performance().result,\n",
    "    explainer_reweight.model_performance().result,\n",
    "    explainer_roc_pivot.model_performance().result\n",
    "], axis=0)"
   ],
   "id": "867c2b8b7f75fba4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
