{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fairness with XGBoost",
   "id": "6a74e87073f9dfca"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import dalex as dx\n",
    "import xgboost\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load data",
   "id": "239d21b0cf928d3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = dx.datasets.load_titanic()\n",
    "\n",
    "X = df.drop(columns='survived')\n",
    "X = pd.get_dummies(X, columns=[\"gender\", \"class\", \"embarked\"], drop_first=True)\n",
    "y = df.survived\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split( X, y, test_size=0.33, random_state=42)"
   ],
   "id": "44912799cc8095a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = xgboost.XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=2,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "\n",
    "    enable_categorical=True,\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ],
   "id": "f470d2efd7e78519",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def pf_xgboost_classifier_categorical(model, df):\n",
    "    df.loc[:, df.dtypes == 'object'] = \\\n",
    "        df.select_dtypes(['object']) \\\n",
    "            .apply(lambda x: x.astype('category'))\n",
    "    return model.predict_proba(df)[:, 1]\n",
    "\n",
    "explainer = dx.Explainer(model, X_test, y_test, predict_function=pf_xgboost_classifier_categorical)"
   ],
   "id": "2fcf07188899c8a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "explainer.model_performance()",
   "id": "da921ab325b70cb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fairness\n",
    "\n",
    "Now we will look at creating a protected group in our dataset, to evaluate the fairness of our model."
   ],
   "id": "4ac3cffd3d49960e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "protected_variable = X_test.gender_male.apply(lambda x: \"male\" if x else \"female\")\n",
    "privileged_group = \"male\"\n",
    "\n",
    "fobject = explainer.model_fairness(\n",
    "    protected=protected_variable,\n",
    "    privileged=privileged_group\n",
    ")"
   ],
   "id": "10dea85645765001",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Bias detection\n",
    "\n",
    "Fairness objects have a convenient form of describing model bias using the fairness_check() method.\n",
    "\n",
    "Several metrics are computed and checked automatically. Here they are, along with whether we want higher or lower.\n",
    "\n",
    "- TPR - True positive rate / Equal opportunity (higher is better)\n",
    "- PPV - Positive predictive value / Predictive parity (higher is better)\n",
    "- FPR - False positive rate / Predictive equality (lower is better)\n",
    "- ACC - Accuracy / Accuracy equality (higher is better)\n",
    "- STP - Statistical parity / Demographic parity (lower is better)\n",
    "\n",
    "For a broad description of these methods, consider refering to the following article and its references:\n",
    "\n",
    "    J. Wi≈õniewski & P. Biecek. fairmodels: a Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models. The R Journal, 2022.\n",
    "\n",
    "More resources are available at https://fairmodels.drwhy.ai and specifically for Python at https://dalex.drwhy.ai/python#fairness.\n"
   ],
   "id": "ee5c14b349fe70a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fobject.fairness_check()",
   "id": "62e30ef26a6e0254",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fobject.plot()",
   "id": "d183ffc46c515e85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We clearly observe high bias towards the privileged group in the model. Let's construct a model without the protected variable.",
   "id": "a3ebc61bd4fefd42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_without_prot, X_test_without_prot = X_train.drop(\"gender_male\", axis=1), X_test.drop(\"gender_male\", axis=1)\n",
    "\n",
    "model_without_prot = xgboost.XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=2,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "    enable_categorical=True,\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "\n",
    "model_without_prot.fit(X_train_without_prot, y_train)\n",
    "\n",
    "explainer_without_prot = dx.Explainer(\n",
    "    model_without_prot,\n",
    "    X_test_without_prot,\n",
    "    y_test,\n",
    "    predict_function=pf_xgboost_classifier_categorical,\n",
    "    label=\"XGBClassifier without the protected attribute\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "fobject_without_prot = explainer_without_prot.model_fairness(protected_variable, privileged_group)"
   ],
   "id": "3481e9f9ab63b752",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fobject.plot(fobject_without_prot, show=False). \\\n",
    "    update_layout(autosize=False, width=800, height=450, legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99))"
   ],
   "id": "8be789c6c9079075",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We managed to improve on 3 fairness metrics, at a cost of worse Predictive parity ratio.\n",
    "\n",
    "This comes at a cost of model performance:"
   ],
   "id": "ac57ecf117ba15da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pd.concat([explainer.model_performance().result, explainer_without_prot.model_performance().result], axis=0)",
   "id": "1c0efd7500670d17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Bias mitigation\n",
    "\n",
    "Can we decrease model bias without decreasing model performance?\n",
    "\n",
    "This is the goal of bias mitigation methods:\n",
    "\n",
    "- resample - returns indices used to pick relevant samples of data\n",
    "- reweight - returns sample (case) weights for model training\n",
    "- roc_pivot - returns the Explainer with a changed y_hat prediction\n",
    "\n",
    "Let's compare all three.\n"
   ],
   "id": "c550816c34ec0005"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dalex.fairness import resample, reweight, roc_pivot\n",
    "from copy import copy\n",
    "\n",
    "protected_variable_train = X_train.gender_male.apply(lambda x: \"male\" if x else \"female\")\n",
    "\n",
    "# resample\n",
    "indices_resample = resample(\n",
    "    protected_variable_train,\n",
    "    y_train,\n",
    "    type='preferential', # uniform\n",
    "    probs=model_without_prot.predict_proba(X_train_without_prot)[:, 1], # requires probabilities\n",
    "    verbose=False\n",
    ")\n",
    "model_resample = copy(model_without_prot)\n",
    "model_resample.fit(X_train_without_prot.iloc[indices_resample, :], y_train.iloc[indices_resample])\n",
    "explainer_resample = dx.Explainer(\n",
    "    model_resample,\n",
    "    X_test_without_prot,\n",
    "    y_test,\n",
    "    label='XGBClassifier with Resample mitigation',\n",
    "    verbose=False\n",
    ")\n",
    "fobject_resample = explainer_resample.model_fairness(\n",
    "    protected_variable,\n",
    "    privileged_group\n",
    ")\n",
    "\n",
    "# reweight\n",
    "sample_weight = reweight(\n",
    "    protected_variable_train,\n",
    "    y_train,\n",
    "    verbose=False\n",
    ")\n",
    "model_reweight = copy(model_without_prot)\n",
    "model_reweight.fit(X_train_without_prot, y_train, sample_weight=sample_weight)\n",
    "explainer_reweight = dx.Explainer(\n",
    "    model_reweight,\n",
    "    X_test_without_prot,\n",
    "    y_test,\n",
    "    label='XGBClassifier with Reweight mitigation',\n",
    "    verbose=False\n",
    ")\n",
    "fobject_reweight = explainer_reweight.model_fairness(\n",
    "    protected_variable,\n",
    "    privileged_group\n",
    ")\n",
    "\n",
    "# roc_pivot\n",
    "explainer_roc_pivot = roc_pivot(\n",
    "    copy(explainer_without_prot),\n",
    "    protected_variable,\n",
    "    privileged_group,\n",
    "    verbose=False\n",
    ")\n",
    "explainer_roc_pivot.label = 'XGBClassifier with ROC pivot mitigation'\n",
    "fobject_roc_pivot = explainer_roc_pivot.model_fairness(\n",
    "    protected_variable,\n",
    "    privileged_group\n",
    ")"
   ],
   "id": "6eeb58790766bfb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fobject_without_prot.plot([fobject_resample, fobject_reweight, fobject_roc_pivot], show=False). \\\n",
    "    update_layout(autosize=False, width=800, height=450, legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99))"
   ],
   "id": "2c666a3aff3bb117",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for fobj in [fobject_without_prot, fobject_resample, fobject_reweight, fobject_roc_pivot]:\n",
    "    print(\"\\n========== \" + fobj.label + \" ==========\")\n",
    "    fobj.fairness_check(epsilon=0.66)"
   ],
   "id": "cea406699b360a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, let's check the bias-performance tradeoff.",
   "id": "2b310015004e87ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.concat([\n",
    "    explainer_without_prot.model_performance().result,\n",
    "    explainer_resample.model_performance().result,\n",
    "    explainer_reweight.model_performance().result,\n",
    "    explainer_roc_pivot.model_performance().result\n",
    "], axis=0)"
   ],
   "id": "867c2b8b7f75fba4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
